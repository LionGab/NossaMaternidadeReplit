/**
 * AI Service - Client Wrapper (Seguro)
 *
 * Chama a Edge Function /ai com JWT do usu√°rio autenticado.
 * Nunca exp√µe API keys no client.
 */

import { AIMessage, AIResponse } from "../types/ai";
import { logger } from "../utils/logger";
import { fetchWithRetry, TIMEOUT_PRESETS } from "../utils/fetch-utils";
import { AppError, ErrorCode, wrapError, isAppError } from "../utils/error-handler";
import { supabase } from "./supabase";
import { rateLimiter } from "../utils/rate-limiter";
import { getSupabaseFunctionsUrl } from "../config/env";
import { chatMessagesSchema, validateWithSchema } from "../utils/validation";
import {
  generateWithAppleFoundationModels,
  isAppleFoundationModelsAvailable,
} from "../ai/appleFoundationModels";

const FUNCTIONS_URL = getSupabaseFunctionsUrl();
const AI_FUNCTION_CANDIDATES = ["ai", "nathia-chat"] as const;

export interface AIContext {
  requiresGrounding?: boolean; // Usa Gemini com Google Search
  estimatedTokens?: number; // Estimativa para rate limiting
  imageData?: {
    // Suporte a imagem (Claude vision)
    base64: string;
    mediaType: string;
  };
  isCrisis?: boolean; // For√ßa Claude para situa√ß√µes de crise
  conversationId?: string; // ID da conversa para persist√™ncia no DB
  preferredProvider?: "apple" | "claude" | "gemini" | "openai";
  preferredModel?: string;
  /** AbortSignal for request cancellation */
  abortSignal?: AbortSignal;
}

/**
 * Palavras-chave de CRISE - for√ßa uso de Claude (modelo mais seguro)
 * Sincronizado com edge function
 */
const CRISIS_KEYWORDS = [
  "suic√≠dio",
  "suicidio",
  "me matar",
  "quero morrer",
  "n√£o quero viver",
  "melhor morta",
  "vou me matar",
  "penso em morrer",
  "acabar com tudo",
  "n√£o aguento mais viver",
  "queria estar morta",
  "machucar o beb√™",
  "machucar meu filho",
  "machucar minha filha",
  "fazer mal ao beb√™",
  "jogar o beb√™",
  "sufocar o beb√™",
  "me cortar",
  "me machucar",
  "me ferir",
  "n√£o tenho sa√≠da",
  "ningu√©m se importa",
  "sou um peso",
];

/**
 * Detecta se mensagem indica crise (requer Claude)
 */
export function detectCrisis(message: string): boolean {
  const lower = message.toLowerCase();
  return CRISIS_KEYWORDS.some((k) => lower.includes(k));
}

/**
 * Payload enviado para a Edge Function /ai
 */
interface EdgeFunctionPayload {
  messages: AIMessage[];
  provider: "claude" | "gemini" | "openai";
  model?: string;
  grounding: boolean;
  imageData?: {
    base64: string;
    mediaType: string;
  };
  conversationId?: string;
}

type AIProvider = "apple" | EdgeFunctionPayload["provider"];

/**
 * Obter resposta da NathIA (Claude ou Gemini)
 * Decide automaticamente o provider com base no contexto
 * Inclui retry + timeout autom√°tico
 */
export async function getNathIAResponse(
  messages: AIMessage[],
  context: AIContext = {}
): Promise<AIResponse> {
  try {
    // Valida√ß√£o com Zod (CR√çTICO: previne prompt injection, mensagens vazias, spam)
    const validation = validateWithSchema(chatMessagesSchema, messages);
    if (!validation.success) {
      const errorMessage = validation.errors.join(", ");
      logger.error("Chat messages validation failed", "AIService", new Error(errorMessage));
      throw new AppError(
        "Mensagens inv√°lidas",
        ErrorCode.VALIDATION_ERROR,
        errorMessage,
        undefined,
        { component: "AIService" }
      );
    }

    // Rate limiting - anti-spam
    if (!rateLimiter.canProceed("nathia-burst")) {
      throw new AppError(
        "Rate limit exceeded (burst)",
        ErrorCode.RATE_LIMITED,
        "Calma! Aguarde alguns segundos antes de enviar outra mensagem.",
        undefined,
        { key: "nathia-burst", resetIn: rateLimiter.getTimeUntilReset("nathia-burst") }
      );
    }

    if (!rateLimiter.canProceed("nathia")) {
      const resetIn = Math.ceil(rateLimiter.getTimeUntilReset("nathia") / 1000);
      throw new AppError(
        "Rate limit exceeded",
        ErrorCode.RATE_LIMITED,
        `Voc√™ atingiu o limite de mensagens. Aguarde ${resetIn}s.`,
        undefined,
        { key: "nathia", resetIn }
      );
    }

    // 1. Verificar autentica√ß√£o
    if (!supabase) {
      throw new AppError(
        "Supabase n√£o est√° configurado",
        ErrorCode.API_ERROR,
        "Erro de configura√ß√£o. Contate o suporte.",
        undefined,
        { component: "AIService" }
      );
    }

    if (!FUNCTIONS_URL) {
      throw new AppError(
        "Supabase Functions URL not configured",
        ErrorCode.API_ERROR,
        "Erro de configura√ß√£o. Contate o suporte.",
        undefined,
        { component: "AIService", missing: "EXPO_PUBLIC_SUPABASE_FUNCTIONS_URL" }
      );
    }

    if (!supabase) {
      throw new AppError(
        "Supabase not configured",
        ErrorCode.API_ERROR,
        "Servi√ßo temporariamente indispon√≠vel. Tente novamente mais tarde.",
        undefined,
        { component: "AIService" }
      );
    }

    const {
      data: { session },
    } = await supabase.auth.getSession();

    if (!session?.access_token) {
      throw new AppError(
        "Session token not found",
        ErrorCode.UNAUTHORIZED,
        "Voc√™ n√£o est√° autenticado. Fa√ßa login para continuar.",
        undefined,
        { component: "AIService" }
      );
    }

    // 2. Decidir provider (NathIA v2.0)
    // Default: OpenAI GPT-4o Mini (estavel e custo-beneficio)
    // Crise/Imagem: Claude (mais seguro)
    let provider: AIProvider = context.preferredProvider || "openai";
    let grounding = false;

    // Detectar crise na √∫ltima mensagem do usu√°rio
    const lastUserMessage = messages.filter((m) => m.role === "user").pop();
    const isCrisis = context.isCrisis || (lastUserMessage && detectCrisis(lastUserMessage.content));

    if (isCrisis) {
      // üö® CRISE: For√ßa Claude (modelo mais seguro para situa√ß√µes delicadas)
      provider = "claude";
      logger.warn("Crisis detected, routing to Claude", "AIService");
    } else if (context.imageData) {
      // Imagem ‚Üí Claude Vision
      provider = "claude";
    } else if (context.requiresGrounding) {
      // Pergunta m√©dica ‚Üí Gemini com Google Search
      provider = "gemini";
      grounding = true;
    } else if (context.estimatedTokens && context.estimatedTokens > 100000) {
      // Long context (>100K tokens) ‚Üí Gemini (1M window)
      provider = "gemini";
    }
    // Default: OpenAI (NathIA v2.1 - estabilidade)

    // 2.1 Provider local (Apple Foundation Models)
    // S√≥ roda quando explicitamente escolhido (preferredProvider="apple") ou quando a UX decidir assim.
    if (provider === "apple") {
      if (!isAppleFoundationModelsAvailable()) {
        logger.warn(
          "Apple Foundation Models requested but unavailable; falling back to OpenAI",
          "AIService"
        );
        provider = "openai";
      } else {
        try {
          const startTime = Date.now();
          const content = await generateWithAppleFoundationModels(messages);
          const latency = Date.now() - startTime;
          return {
            content,
            provider: "apple",
            latency,
            // N√£o temos contagem de tokens do modelo on-device; manter 0 para compatibilidade
            usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
          };
        } catch (appleError) {
          logger.warn("Apple Foundation Models failed; falling back to OpenAI", "AIService", {
            error: appleError instanceof Error ? appleError.message : String(appleError),
          });
          provider = "openai";
        }
      }
    }

    // 3. Preparar payload
    const payload: EdgeFunctionPayload = {
      messages,
      provider,
      grounding,
      ...(context.preferredModel && { model: context.preferredModel }),
      ...(context.imageData && { imageData: context.imageData }),
      ...(context.conversationId && { conversationId: context.conversationId }),
    };

    // 4. Chamar Edge Function COM JWT + RETRY + TIMEOUT
    // Compatibilidade: tenta endpoint can√¥nico "ai" e depois "nathia-chat" (legado)
    let response: Response | null = null;
    let lastEndpointError: unknown = null;

    for (const endpoint of AI_FUNCTION_CANDIDATES) {
      const targetUrl = `${FUNCTIONS_URL}/${endpoint}`;
      try {
        response = await fetchWithRetry(
          targetUrl,
          {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: `Bearer ${session.access_token}`,
              "x-client-platform": "expo-app",
              "x-client-version": "2.0.0",
            },
            body: JSON.stringify(payload),
            // AI pode demorar at√© 60s dependendo do provider
            timeoutMs: TIMEOUT_PRESETS.CRITICAL,
            context: `AIService:${endpoint}`,
            // Pass external abort signal for user cancellation
            abortSignal: context.abortSignal,
          },
          {
            // N√£o retry em erros de autentica√ß√£o/valida√ß√£o
            maxAttempts: 3,
            initialDelay: 1000,
            maxDelay: 5000,
            retryable: (error) => {
              if (isAppError(error)) {
                // N√£o retry em erros que n√£o devem ser retentados
                return ![
                  ErrorCode.UNAUTHORIZED,
                  ErrorCode.FORBIDDEN,
                  ErrorCode.INVALID_INPUT,
                ].includes(error.code as ErrorCode);
              }
              return true;
            },
          }
        );
        if (endpoint !== "ai") {
          logger.warn("Using legacy AI endpoint fallback", "AIService", { endpoint });
        }
        break;
      } catch (error) {
        lastEndpointError = error;
        const status =
          isAppError(error) && typeof error.context?.status === "number"
            ? (error.context.status as number)
            : null;
        if (status === 404 || status === 405) {
          continue;
        }
        throw error;
      }
    }

    if (!response) {
      throw wrapError(
        lastEndpointError || new Error("No AI endpoint reachable"),
        ErrorCode.API_ERROR,
        "Servi√ßo da NathIA indispon√≠vel. Tente novamente em instantes.",
        { endpointsTried: AI_FUNCTION_CANDIDATES, component: "AIService" }
      );
    }

    // 5. Parse resposta
    const data = await response.json();

    // Validar estrutura da resposta
    if (!data.content || !data.usage || !data.provider) {
      throw new AppError(
        "Invalid response structure from AI service",
        ErrorCode.API_ERROR,
        "Resposta inv√°lida do servidor. Tente novamente.",
        undefined,
        { received: Object.keys(data) }
      );
    }

    logger.info(
      `AI response: ${data.provider}, ${data.latency}ms, ${data.usage.totalTokens} tokens`,
      "AIService"
    );

    // Aviso se usou fallback
    if (data.fallback) {
      logger.warn("AI fallback: Claude offline, usou OpenAI", "AIService");
    }

    return {
      content: data.content,
      usage: data.usage,
      provider: data.provider,
      latency: data.latency,
      grounding: data.grounding,
      fallback: data.fallback,
    };
  } catch (error) {
    // Se j√° √© AppError, re-throw mantendo contexto
    if (isAppError(error)) {
      throw error;
    }

    // Converter erro gen√©rico para AppError
    throw wrapError(
      error,
      ErrorCode.AI_SERVICE_ERROR,
      "N√£o consegui processar sua mensagem. Tente novamente em instantes.",
      { component: "AIService", messageCount: messages.length }
    );
  }
}

/**
 * Estimar tokens de uma conversa (rough approximation)
 * ~4 chars = 1 token
 */
export function estimateTokens(messages: AIMessage[]): number {
  const totalChars = messages.reduce((sum, m) => sum + m.content.length, 0);
  return Math.ceil(totalChars / 4);
}

/**
 * Detectar se √© pergunta m√©dica (requires grounding)
 * Palavras-chave que indicam necessidade de busca atualizada
 */
export function detectMedicalQuestion(message: string): boolean {
  const medicalKeywords = [
    "o que √©",
    "como prevenir",
    "sintomas de",
    "tratamento para",
    "pode ser",
    "√© normal",
    "pesquisar",
    "informa√ß√µes sobre",
    "dados sobre",
    "estudos sobre",
    // Termos m√©dicos comuns
    "pr√©-ecl√¢mpsia",
    "ecl√¢mpsia",
    "diabetes gestacional",
    "hipertens√£o",
    "anemia",
    "infec√ß√£o urin√°ria",
    "contra√ß√µes",
    "placenta",
    "l√≠quido amni√≥tico",
  ];

  const lowerMessage = message.toLowerCase();
  return medicalKeywords.some((keyword) => lowerMessage.includes(keyword));
}

/**
 * Converter imagem URI para base64 (para Claude vision)
 */
export async function imageUriToBase64(
  uri: string
): Promise<{ base64: string; mediaType: string }> {
  try {
    // Se j√° √© base64, extrair
    if (uri.startsWith("data:")) {
      const match = uri.match(/^data:(image\/\w+);base64,(.*)$/);
      if (!match) throw new Error("Invalid base64 data URI");
      return {
        mediaType: match[1],
        base64: match[2],
      };
    }

    // Sen√£o, fetch e converter
    const response = await fetch(uri);
    const blob = await response.blob();
    const reader = new FileReader();

    return new Promise((resolve, reject) => {
      reader.onloadend = () => {
        const dataUrl = reader.result as string;
        const match = dataUrl.match(/^data:(image\/\w+);base64,(.*)$/);
        if (!match) {
          reject(new Error("Failed to convert image to base64"));
          return;
        }
        resolve({
          mediaType: match[1],
          base64: match[2],
        });
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  } catch (error) {
    logger.error("Image conversion error", "AIService", error as Error);
    throw new Error("N√£o consegui processar a imagem. Tente outra.");
  }
}
